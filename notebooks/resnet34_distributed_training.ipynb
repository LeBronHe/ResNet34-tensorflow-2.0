{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.0.0-beta0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('TensorFlow', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9b98607cc0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "starter_learning_rate = 1e-4\n",
    "end_learning_rate = 1e-5\n",
    "decay_steps = 10000\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    starter_learning_rate,\n",
    "    decay_steps,\n",
    "    end_learning_rate,\n",
    "    power=0.5)\n",
    "x = [learning_rate_fn(i).numpy() for i in range(21000)]\n",
    "plt.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.RING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(input_tensor, block_type, n_filters):\n",
    "    shortcut = input_tensor\n",
    "    if block_type == 'conv':\n",
    "        strides = 2\n",
    "        shortcut =  tf.keras.layers.Conv2D(filters=n_filters, \n",
    "                                           kernel_size=1, \n",
    "                                           padding='same',\n",
    "                                           strides=strides,\n",
    "                                           kernel_initializer='he_normal', \n",
    "                                           use_bias=False,\n",
    "                                           kernel_regularizer=tf.keras.regularizers.l2(l=1e-4))(shortcut)\n",
    "        shortcut = tf.keras.layers.BatchNormalization(momentum=0.9)(shortcut)\n",
    "        \n",
    "    elif block_type == 'identity':\n",
    "        strides = 1\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters=n_filters, \n",
    "                               kernel_size=3, \n",
    "                               padding='same',\n",
    "                               strides=strides,\n",
    "                               kernel_initializer='he_normal', \n",
    "                               use_bias=False, \n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l=1e-4))(input_tensor)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(filters=n_filters, \n",
    "                               kernel_size=3, \n",
    "                               padding='same',\n",
    "                               strides=1,\n",
    "                               kernel_initializer='he_normal', \n",
    "                               use_bias=False, \n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l=1e-4))(x)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "    x = tf.keras.layers.Add()([x, shortcut])\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def ResNet34(input_shape=[None, None, 3], num_classes=1000, include_top=True, return_endpoints=False):\n",
    "    input_tensor = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv2D(filters=64, \n",
    "                               kernel_size=7, \n",
    "                               padding='same', \n",
    "                               strides=2, \n",
    "                               kernel_initializer='he_normal', \n",
    "                               use_bias=False, \n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l=1e-4))(input_tensor)\n",
    "    x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "    x = tf.keras.layers.MaxPool2D(3, 2, padding='same')(x)\n",
    "    \n",
    "    for n_filters, reps, downscale in zip([64, 128, 256, 512], \n",
    "                                          [3, 4, 6, 3], \n",
    "                                          [False, True, True, True]):\n",
    "        for i in range(reps):\n",
    "            if i == 0 and downscale:\n",
    "                x = residual_block(input_tensor=x, \n",
    "                                   block_type='conv', \n",
    "                                   n_filters=n_filters)\n",
    "            else:\n",
    "                x = residual_block(input_tensor=x, \n",
    "                                   block_type='identity', \n",
    "                                   n_filters=n_filters)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    if include_top:\n",
    "        x = tf.keras.layers.Dense(units=num_classes)(x)\n",
    "    return tf.keras.Model(inputs=input_tensor, outputs=x, name='ResNet34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes :  10\n",
      "Found 12894 train images\n",
      "Found 500 validation images\n"
     ]
    }
   ],
   "source": [
    "H, W = 256, 256\n",
    "classes = sorted(['n03417042', 'n03028079', 'n03888257', 'n02102040', 'n01440764', 'n03445777', 'n03000684', 'n02979186', 'n03394916', 'n03425413'])\n",
    "label_map = {v:i for i, v in enumerate(classes)}\n",
    "\n",
    "train_images = glob('imagenette/train/*/*')\n",
    "np.random.shuffle(train_images)\n",
    "train_labels = [label_map[x.split('/')[-2]] for x in train_images]\n",
    "\n",
    "val_images = glob('imagenette/val/*/*')\n",
    "np.random.shuffle(val_images)\n",
    "val_labels = [label_map[x.split('/')[-2]] for x in val_images]\n",
    "\n",
    "print('Number of classes : ', len(classes))\n",
    "print('Found', len(train_images), 'train images')\n",
    "print('Found', len(val_images), 'validation images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.random.shuffle(val_images).numpy()\n",
    "train_labels = [label_map[x.decode().split('/')[-2]] for x in train_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = tf.constant([0., 0., 1., 1.])\n",
    "bbox = tf.reshape(bbox, shape=[-1, 1, 4])\n",
    "channel_means =  tf.constant([103.939, 116.779, 123.68])\n",
    "\n",
    "def get_image(image_path, H=H, W=W):\n",
    "    with tf.device('/cpu:0'):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
    "        tf.image.extract_jpeg_shape(img),\n",
    "        bounding_boxes=bbox,\n",
    "        min_object_covered=0.1,\n",
    "        aspect_ratio_range=[0.75, 1.33],\n",
    "        area_range=[0.05, 1.0],\n",
    "        max_attempts=100,\n",
    "        use_image_if_no_bounding_boxes=True)\n",
    "        bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
    "\n",
    "        offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
    "        target_height, target_width, _ = tf.unstack(bbox_size)\n",
    "        crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n",
    "\n",
    "        cropped = tf.image.decode_and_crop_jpeg(\n",
    "          img, crop_window, channels=3)\n",
    "        cropped = tf.cast(cropped, dtype=tf.float32)\n",
    "        cropped = tf.image.random_flip_left_right(cropped)\n",
    "        cropped.set_shape([None, None, 3])            \n",
    "\n",
    "        processed_image = tf.image.resize(cropped, size=[H, W], method='bilinear')\n",
    "        processed_image = tf.clip_by_value(processed_image, 0, 255)\n",
    "        processed_image  = processed_image[:,:,::-1] - channel_means\n",
    "    return processed_image\n",
    "\n",
    "def resize_preserve_aspect_ratio(image_tensor, min_side_dim=H):\n",
    "    img_h, img_w = image_tensor.shape.as_list()[:2]\n",
    "    min_dim = tf.minimum(img_h, img_w)\n",
    "    resize_ratio = min_side_dim / min_dim\n",
    "    new_h, new_w = resize_ratio * img_h, resize_ratio * img_w\n",
    "    resized_image_tensor = tf.image.resize(image_tensor, size=[new_h, new_w])\n",
    "    return resized_image_tensor\n",
    "    \n",
    "def get_image_eval(image_path, H=H, W=W):\n",
    "    with tf.device('/cpu:0'):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_image(img, channels=3)\n",
    "        img = resize_preserve_aspect_ratio(img, min_side_dim=H)\n",
    "        img = tf.image.resize_with_crop_or_pad(img, H, W)\n",
    "        img  = img[:,:,::-1] - channel_means\n",
    "    return img\n",
    "\n",
    "def prepare_data(training=False):\n",
    "    def load_data(image_path, label):\n",
    "        if training:\n",
    "            image = get_image(image_path)\n",
    "        else:\n",
    "            image = get_image_eval(image_path)\n",
    "        return image, label\n",
    "    return load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 * 3\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(train_images) + 1)\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.map_and_batch(map_func=load_data, \n",
    "                                                   batch_size=batch_size, \n",
    "                                                   num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "                                                   drop_remainder=True))\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
    "val_dataset = val_dataset.apply(tf.data.experimental.map_and_batch(map_func=load_data,\n",
    "                                                   batch_size=batch_size, \n",
    "                                                   num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "                                                   drop_remainder=True))\n",
    "val_dataset = val_dataset.repeat()\n",
    "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('top_weights.h5', \n",
    "                                       save_best_only=True, \n",
    "                                       save_weights_only=True,\n",
    "                                       monitor='val_sparse_categorical_accuracy'),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir='./logs', update_freq='batch')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter_learning_rate = 1e-3\n",
    "end_learning_rate = 5e-5\n",
    "decay_steps = 60000\n",
    "learning_rate_fn = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    starter_learning_rate,\n",
    "    decay_steps,\n",
    "    end_learning_rate,\n",
    "    power=0.8)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate_fn)\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy(['/gpu:0', '/gpu:1', '/gpu:2'])\n",
    "with strategy.scope():\n",
    "    model = ResNet34(input_shape=[H, W, 3], num_classes=10, include_top=True)\n",
    "    model.compile(loss=loss, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[tf.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "model.fit(train_dataset,\n",
    "          steps_per_epoch=len(train_images) // batch_size,\n",
    "          epochs=400,\n",
    "          validation_data=val_dataset,\n",
    "          validation_steps=len(val_images) // batch_size,\n",
    "          callbacks=callbacks_list)\n",
    "model.save_weights('last_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ResNet34(input_shape=[H, W, 3], num_classes=10, include_top=True)\n",
    "model.load_weights('last_epoch.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ca3c686b654ac2a71d74df1ad2d976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95        50\n",
      "           1       1.00      0.96      0.98        50\n",
      "           2       0.94      0.94      0.94        50\n",
      "           3       0.98      0.92      0.95        50\n",
      "           4       0.82      0.94      0.88        50\n",
      "           5       0.79      0.88      0.83        50\n",
      "           6       0.92      0.96      0.94        50\n",
      "           7       0.96      0.90      0.93        50\n",
      "           8       0.98      0.86      0.91        50\n",
      "           9       0.94      0.94      0.94        50\n",
      "\n",
      "    accuracy                           0.92       500\n",
      "   macro avg       0.93      0.92      0.93       500\n",
      "weighted avg       0.93      0.92      0.93       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels = []\n",
    "for image_path in tqdm_notebook(val_images):\n",
    "    img = get_image_eval(image_path)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    logits = model(img)[0]\n",
    "    label = tf.argmax(logits, axis=-1).numpy()\n",
    "    pred_labels.append(label)\n",
    "print(classification_report(val_labels, pred_labels))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
